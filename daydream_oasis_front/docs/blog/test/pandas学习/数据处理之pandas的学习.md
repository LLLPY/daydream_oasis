
<BlogInfo title="数据处理之pandas的学习" author="白日梦想猿" pv=0 read_times=0 pre_cost_time=575 category="pandas学习" tag_list="['数据处理', 'pandas']" create_time="2021.08.28 17:45:29.372680" update_time="2021.08.28 18:19:38" />

^^^^^^^^^
<h2>1.pandas中的几种数据结构</h2><pre>import pandas as pd<br>import numpy as np<br><br># 数据结构<br>'''<br>pandas中一共有三种数据结构,分别为:Series,DataFrame和MultiIndex<br>其中Series是一维数据结构,DataFrame是二维数据表结构,MultiIndex是三维数据结构<br>'''<br><br># 1.Series<br>'''<br>Series是一个类似于一维数组的数据结构,它能够保存任何类型的数据,比如浮点,整数等<br>主要由一组数据和与之相关联的索引两部分组成<br><br>API:<br>    pd.Series(data=None,index=None,dtype=None)<br>    参数:<br>        .data:传入的数据,可以是ndarray,list等<br>        .index:索引,必须是唯一的,且与数据的长度保持一致,如果没有传入索引参数,<br>         则会默认创建一个0~N的整数索引<br>        .dtype:数据类型<br><br>Series的属性<br>    Series.index:获取索引<br>    Series.values:获取数据值<br>    <br>'''<br><br>a = [i for i in range(10, 1000, 10)]<br><br># 通过数组数据创建<br>a_series = pd.Series(a, )<br>print(f'a={a}]
a_Series=
{a_series}')<br><br># 通过字典数据创建<br>a_dict = {'a': 1, 'b': 2, 'c': 3}<br>a_dict_Series = pd.Series(a_dict)<br>print(f'a_dict={a_dict}
a_dict_Series=
{a_dict_Series}')<br><br># 获取索引<br>a_dict_index = a_dict_Series.index<br>print(f'a_dict_index={a_dict_index}')<br><br># 获取数据值<br>a_dict_values = a_dict_Series.values<br>print(f'a_dict_values={a_dict_values}')<br><br># 通过索引获取值<br>print(a_dict_Series['a'])<br><br># 2.DataFrame<br>'''<br>DataFrames是一个类似于二维数组或表格的对象,既有行索引,也有列索引<br>.行索引,表名不同行,横线索引,叫index,0轴,axis=0<br>.列索引,表名不同列,纵向索引,加columns,1轴,axis=1<br><br>API:<br>    pd.DataFrame(data=None,index=None,columns=None)<br>    参数:<br>        .index:行标签,如果没有传入索引参数,则会自动创建一个从0~N的整数索引<br>        .columns:列标签,如果没有传入索引参数,则会自动创建一个从0~N的整数索引<br><br>属性:<br>    DataFrame.shape:返回[index,columns]<br>    DataFrame.index:返回行索引<br>    DataFrame.columns:返回列索引<br>    DataFrame.values:返回数据值<br>    DataFrame.T:转置,行列互换<br>    DataFrame.head(n=5):显示前n行,如果不输入n,默认显示前5行<br>    DataFrame.tail(n=5):显示后n行,如果不输入n,默认显示后5行<br><br>索引操作:<br>    修改索引:<br>        DataFrame.index=newIndex<br>        注:索引必须整体进行修改,不能只修改其中某一个索引<br>        DataFrame.index[3]=newItemIndex(错误!!!)<br>        <br>        1.设置索引<br>        DataFrame.reset_index(drop=False)<br>        参数:<br>            drop:默认为False,不删除原来的索引,如果为True则删除,即用新索引替换<br>                 旧索引<br>                 <br>        2.以某列值设为新的索引<br>        DataFrame.set_index(keys,drop=True)<br>        参数:<br>            .keys:列索引名称或列索引名称列表<br>            .drop:默认删除原来的列<br><br>'''<br><br>b = np.random.randn(3, 3)  # 3*3的数组<br>b_DataFrame = pd.DataFrame(b)<br>print(f'b=
{b}
b_DataFrame=
{b_DataFrame}')<br>print(f'b_DataFrame.shape={b_DataFrame.shape}')<br>print(f'b_DataFrame.index={b_DataFrame.index}')<br>print(f'b_DataFrame.columns={b_DataFrame.columns}')<br>print(f'b_DataFrame.values=
{b_DataFrame.values}')<br>b_DataFrame_T = b_DataFrame.T<br>print(f'转置后的:
{b_DataFrame_T}')<br>print(f'显示前2行:
{b_DataFrame.head(2)}')<br>print(f'显示后2行:
{b_DataFrame.tail(2)}')<br><br>data_test = np.array(<br>    [<br>        [2010, 1, 120],<br>        [2010, 2, 124],<br>        [2010, 3, 123],<br>        [2010, 4, 121],<br>        [2012, 5, 167],<br>        [2012, 6, 18],<br>        [2013, 7, 20],<br>        [2013, 8, 120],<br><br>    ]<br>)<br><br>data_test_index = [f'行{i}' for i in range(8)]<br>data_test_columns = ['年', '月', '数']<br>data_test_DataFrame = pd.DataFrame(data_test, index=data_test_index, columns=data_test_columns)<br><br>print(f'data_test_DataFrame=
{data_test_DataFrame}')<br><br># 将年这一列的数据设为索引 同时不删除原来的列<br>year_index = data_test_DataFrame.set_index('年', drop=False)<br>print(f'year_index=
{year_index}')<br><br># MultiIndex<br>'''<br>MultiIndex是三维的数据结构,多级索引(也称层次化索引),是pandas的重要功能,可以在<br>Series.DataFrame对象上拥有2个以及2个以上的索引<br>'''<br><br># 创建MultiIndex<br>test_array = [[1, 2, 3], ['r', 'g', 'b']]<br>multiLndex = pd.MultiIndex.from_arrays(test_array,names=('num','col'))<br>print(multiLndex)<br></pre><h2><a></a>2.DataFrame的运算</h2><pre>import pandas as pd<br><br># 读取数据<br>data = pd.read_csv('demo.csv')<br><br># 算术运算<br># 加法<br>print(data['序号'].head())<br>print(data['序号'].head().add(10))  # 给序号这一列的所有数据都加10<br>print(data['序号'].head() + 10)<br><br># 逻辑运算<br># 例 筛选出序号值大于560195290043的数据<br>print('*' * 50)<br>data2 = data[(data['序号'] &gt; 560195290043) &amp; (data['店铺的url'] != '空')].head()<br>print(data2)<br><br># 逻辑运算函数<br>print('*' * 100)<br>data3 = data.query("序号&gt;560195290043 &amp; 店铺的url!='空'").head()  # 筛选结果同上<br>print(data3)<br><br>print('*' * 100)<br># isin<br>data4 = data['价格'].isin([576, '576', 380]).head()<br>print(data4)<br><br># 统计运算<br>print(data.describe())<br><br>print('*' * 100)<br># 统计函数<br>print(data['序号'].sum())  # sum求和<br>print(data['序号'].mean())  # mean平均值<br>print(data['序号'].median())  # median中位数<br>print(data['序号'].min())  # min最小值<br>print(data['序号'].max())  # max最大值<br>print(data['序号'].mode())  # mode众数<br>print(data['序号'].abs())  # abs绝对值<br>print('乘积', data['序号'].prod())  # prod乘积<br>print(data['序号'].std())  # std标准差<br>print(data['序号'].var())  # var方差<br>print(data['序号'].idxmax())  # idxmax最大值的索引值<br>print(data['序号'].idxmin())  # idxmin最小值的索引值<br><br># 对于单个函数进行统计的时候,坐标轴还是按默认"Columns"(axis=0,default) 如果要对行"index"需要指定axis=1<br></pre><h2><a></a>3.累计统计函数</h2><pre>import pandas as pd<br>import matplotlib.pyplot as plt<br>from pprint import pprint<br>data=pd.read_csv('demo.csv')<br><br><br>#计算前n个数的和 cumsum<br>data2=data['序号'].cumsum()<br>print(data2)<br><br>data2.plot()<br>plt.show()<br><br>#cummax 计算前n个数的最大值<br>#cummin 计算前n个数的最小值<br>#cumprod #计算前n个数的乘积<br><br>#自定义函数<br>''':cvar<br>语法:<br>apply(func,axis=0)<br>    .func:自定义的函数<br>    .axis:默认是列,axis=1指定为行<br>'''<br>#例:定义一个对列求最大值-最小值的函数<br>def max_min(x):<br>    # return x.max(x)-x.min(x)<br>    return x*2<br><br>data['2*序号']=data['序号'].apply(lambda x:max_min(x),0)<br>pprint(f'data=
{data}')<br><br><br></pre><h2><a></a>4.文件的读取和存储</h2><p>import pandas as pd</p><p>#读取csv文件<br>data=pd.read_csv(‘demo.csv’,usecols=[‘名称’,‘价格’]) #usecols指定要读取的列<br>print(data)</p><p>#写入文件<br>‘’’:cvar<br>to_csv(path_or_buf=None,sep=",",columns=None,header=True,index=True,mode=“w”,encoding=None)<br>path_or_buf:文件路径<br>sep:分隔符,默认使用","隔开<br>columns:选择需要的列索引<br>header:是否写进列索引值<br>index:是否写进行索引值<br>mode:文件写入的方式 w:重写 a:追加<br>encoding:编码方式<br>‘’’</p><p>#会发现将索引存入文件当中,变成单独的一列数据,如果需要删除,可以指定参数index=False即可<br>data[:10].to_csv(‘demo2.csv’,columns=[‘名称’,‘价格’],index=False)</p><h2><a></a>hdf5文件的存储</h2><pre>import pandas as pd<br><br>''':cvar<br>HDF5文件的读取和存储需要指定一个键,值为要存储的DataFrame<br>pandas.read_hdf(path_or_buf,key=None,**kwargs)<br><br>    从h5文件中读取数据<br>    .path_or_buf:文件路径<br>    .key:读取的键<br>'''<br><br>#一般情况下 读取hdf5文件需要依赖tables库 可以先提前安装好 pip install tables<br><br>data=pd.read_hdf('')<br><br>#存储文件<br>data.to_hdf(path_or_buf='',key='123')<br><br>#再次读取的时候,需要指定键的名字<br>new_data=pd.read_hdf('',key='123')<br><br><br>#注意:<br>'''<br>优先选择使用hdf5文件存储<br>.hdf5在存储的时候支持压缩,使用的方式是blosc,这个是速度最快也是pandas默认支持的<br>.使用压缩可以提高磁盘的利用率,节省空间<br>.hdf5还是跨平台的,可以轻松迁移到Hadoop上面<br><br>'''</pre><p>‘’’</p><h2><a></a>json文件的存储</h2><p>import pandas as pd</p><p>‘’’<br>pandas.read_json(path_or_buf=None,orient=None,typ=‘frame’,lines=False)<br>orient<br>‘’’</p><h1><a></a>读取json</h1><pre>import pandas as pd<br><br>'''<br>pandas.read_json(path_or_buf=None,orient=None,typ='frame',lines=False)<br>    orient<br>'''<br><br># 读取json<br>data = pd.read_json('data.json', orient='records', lines=False)<br>print(data)<br><br># 保存json orient:指定json文件的存储格式,可选参数有:split records index columns values<br>data.to_json('demo3.json', orient='records', lines=True)<br>#lines:是否存储在多行 默认lines=False是存储在一行</pre><h2><a></a>5.缺失值的处理</h2><pre>import pandas as pd<br>import numpy as np<br>#处理缺失值的方法<br>'''<br>.获取缺失值的表示方式(NaN或者其他标记方式)<br>.如果缺失值的标记方式是NaN<br>    判断数据中是否包含NaN:<br>        pd.isnull(df)<br>        pd.notnull(df)<br>    <br>    存在缺失值:<br>        1.删除存在的缺失值:dropna(axis='rows') #默认删除存在缺失值的行<br>            注:不会修改原数据,需要接受返回值<br>        2.替换缺失值:fillna(value,inplace=True) 无返回值<br>            value:替换成的值<br>            inplace:True:会修改原数据,False:不替换修改原数据,生成新的对象<br>.如果缺失值没有使用NaN标记,比如使用"?"<br>    先替换"?"为np.nan,然后继续处理<br><br><br>'''<br><br>data=pd.read_csv('demo2.csv')<br><br>print(type(data))<br><br>#判断有误缺失值<br>isNull=np.any(pd.isnull(data)) #isnull:如果有一个缺失值就会返回True<br>isNull2=np.all(pd.notnull(data)) #notnull:如果有一个缺失值就会返回False<br>print(pd.isnull(data))<br>print(pd.notnull(data))<br>print(isNull)<br>print(isNull2)<br><br>#删除缺失项<br># data2=data.dropna(axis=1) #axis=0:默认删除这一行 axis=1删除这一列<br># print(data2)<br><br>#替换缺失值 例:用平均值替换掉缺失值<br>price_mean=data['价格'].mean()<br>data.fillna(price_mean,inplace=True)<br>print(data.index)<br><br>for i in data.columns:<br>    print(data[i])</pre><h2><a></a>6.数据离散化</h2><pre>import pandas as pd<br><br>#API:pd.qcut(data,q)<br>'''<br>对数据进行分组,一般会与value_counts搭配使用,统计每组的个数<br>'''<br><br>#API:pd.cut(data,bins)<br>'''<br>自定义区间分组<br>'''<br><br>#API:Series.value_counts()<br>'''<br>计算分到每个组数据个数<br>'''<br><br>#读取数据<br>df=pd.read_csv('stock_day.csv')<br><br>#使用turnover列的数据<br>turnover=df['turnover']<br><br>#自动分组(均匀分组)<br>auto_cut=pd.qcut(turnover,10)<br>#查看每组的数量<br>bins_count=auto_cut.value_counts()<br>print(bins_count)<br><br>#自定义分组<br>bins=[0,1,3,5,7,9]<br>hand_cut=pd.cut(turnover,bins)<br>#查看每组的数量<br>bins_count2=hand_cut.value_counts()<br>print(bins_count2)<br><br>#one-hot编码<br>#把每个类别生成一个布尔列,这些列中只有一列可以为这个样本取值为1,其又别称为热编码<br>#API:pd.get_dummies(data,prefix=None)<br>'''<br>参数:<br>    data:array-like,Series,DataFrame<br>    prefix:分组名字<br>'''<br><br>dummies=pd.get_dummies(turnover,prefix='turnover')<br>print(dummies.head())<br><br></pre><h2><a></a>7.数据合并</h2><pre>import pandas as pd<br><br># 如果数据是有多张表组成,那么有时候需要将不同的内容合并在一起分析<br><br><br># API:pd.concat([data1,data2],axis=1)<br>'''<br>    .data1,data2:需要合并的数据<br>    .axis:合并方向默认为行索引<br>'''<br><br>data1 = pd.read_csv('stock_day.csv')<br>data2 = pd.read_csv('demo2.csv')<br><br>data = pd.concat([data1, data2], axis=1)  #<br>print(data)<br><br># API:pd.merge(left,right,how='inner',on=None)<br>'''<br>可以指定按照两组数据的共同键值对合并或者左右各自<br>参数:<br>    left:DataFrame<br>    right:另一个FataFrame<br>    on:指定的共同键<br>    how:按照什么方式连接<br>'''<br><br>left = pd.DataFrame({'key1': ['K0', 'K0', 'K1', 'K2'],<br>                     'key2': ['K0', 'K1', 'K0', 'K1'],<br>                     'A': ['A0', 'A1', 'A2', 'A3'],<br>                     'B': ['B0', 'B1', 'B2', 'B3']})<br><br>right = pd.DataFrame({'key1': ['K0', 'K1', 'K1', 'K2'],<br>                      'key2': ['K0', 'K0', 'K0', 'K0'],<br>                      'C': ['C0', 'C1', 'C2', 'C3'],<br>                      'D': ['D0', 'D1', 'D2', 'D3']})<br><br># 左连接<br>left_connect = pd.merge(left, right, how='left', on=['key1', 'key2'])<br>print(left_connect)<br><br># 右连接<br>right_connect = pd.merge(left, right, how='right', on=['key1', 'key2'])<br>print(right_connect)<br><br># 内连接 默认连接方式是内连接<br>inner_connect = pd.merge(left, right, how='inner', on=['key1', 'key2'])<br>print(inner_connect)<br># 外连接<br>outer_connect = pd.merge(left, right, how='outer', on=['key1', 'key2'])<br>print(outer_connect)<br></pre><h2><a></a>8.交叉表和透视表</h2><pre>import pandas as pd<br>import numpy as np<br>import matplotlib.pyplot as plt<br># 交叉表<br>'''<br>交叉表用于计算一列数据对于另外一列数据的分组个数(用于统计分组频率的特殊透视表)<br>API:<br>    pd.crosstab(value1,value2)<br><br>'''<br><br># 透视表<br>'''<br>透视表是将原有的DataFrame的列分别作为行索引和列索引,然后对指定的列应用聚合函数<br>'''<br><br>df = pd.read_csv('stock_day.csv')<br><br>print(df.index)<br><br># 将数据的列索引转化成对应的日期之后再转换成星期<br>week = pd.to_datetime(df.index).weekday<br>df['week'] = week<br>print(df['p_change'])<br><br># 把p_change按照分为大于0和小于0的<br>p_n_data = np.where(df['p_change'] &gt; 0, 1, 0)<br>df['p_n']=p_n_data<br>print(df)<br><br><br><br>#使用透视表实现 方法1<br>per_count2=df.pivot_table(['p_n'],index='week')<br>print(per_count2)<br><br><br>#方法2<br>#通过交叉表寻找两列数据的关系<br>count=pd.crosstab(week,p_n_data)<br>print(count)<br><br>#计算每个星期各自的总和 axis=1指定为行 astype转换为浮点型<br>sum_count=count.sum(axis=1).astype(np.float32)<br>print(sum_count)<br><br>#求百分比<br>per_count=count.div(sum_count,axis=0)<br>print(per_count)<br><br><br><br>#图片显示 stacked=True:堆积显示<br>per_count.plot(kind='bar',stacked=True)<br>plt.show()<br><br></pre><p data-we-empty-p=""><img src="https://img-blog.csdnimg.cn/4bf3a59faf1d459a839dd393dec69f57.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbGl0dGxl5Lqu772e,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br><br></p><h2><a></a>9.分组与聚合</h2><pre>import pandas as pd<br><br># 分组API<br>'''<br>df.groupby(key,as_index=False)<br>    参数:<br>        key:分组的列数据,可以多个<br>        as_index:时候保留原列的数据,默认不保存<br><br><br>'''<br><br>col = pd.DataFrame(<br>    {'color': ['white', 'red', 'green', 'red', 'green'], 'object': ['pen', 'pencil', 'pencil', 'ashtray', 'pen'],<br>     'price1': [5.56, 4.20, 1.30, 0.56, 2.75], 'price2': [4.75, 4.12, 1.60, 0.75, 3.15]})<br><br>print(col)<br># 方法1 按照color进行分组 分别求每组color下其他各列对应的最大值<br>df1 = col.groupby(['color'],as_index=True).max()<br><br># 方法2 object进行分组 分别求每组object下price1下的最小值<br>df2 = col['price1'].groupby(col['object']).min()<br>print(df1)<br>print(df2)<br></pre><h2><a></a>10.案例一(星巴克)</h2><pre>import pandas as pd<br>import matplotlib.pyplot as plt<br><br>data=pd.read_csv('directory.csv')<br># print(data.head())<br># print(data.columns.values)<br><br>#统计各个国家的星巴克数量<br>country_count=data.groupby(['Country']).count()['Brand']<br><br>country_count.plot(kind='bar',figsize=(20,8),)<br>plt.xlabel('country')<br>plt.ylabel('num')<br>plt.title('country-number')<br>plt.show()<br><br>#统计各个省份/州的星巴克数量<br>province_count=data.groupby(['Country','State/Province']).count()['Brand']<br>province_count.plot(kind='bar',figsize=(200,8))<br>plt.show()</pre><p data-we-empty-p=""><img src="https://img-blog.csdnimg.cn/a305f7ef1849417cb74c12edb9be64f4.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbGl0dGxl5Lqu772e,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br></p><p data-we-empty-p=""><br></p><h2><a></a>11.案例二(电影)</h2><pre>import numpy as np<br>import pandas as pd<br>import matplotlib.pyplot as plt<br><br>#读取数据<br>data=pd.read_csv('IMDB-Movie-Data.csv')<br>print(data.head().columns)<br><br>#1.获取所有电影的平均分<br>Rating_mean=data['Rating'].mean()<br>print(f'平均分:{Rating_mean}')<br><br>#导演的总人数<br>director_num=np.unique(data['Director']).size<br>print(f'导演的总人数:{director_num}')<br><br>#查看每个导演的作品数量<br>director_work_num=data.groupby(['Director']).count()['Title'].sort_values()<br>director_work_num.plot(kind='bar',figsize=(200,8))<br>plt.show()<br><br><br>#获取Rating和runtime的分布情况<br>rating_num=data['Rating']<br>runtime_num=data['Runtime (Minutes)']<br>plt, axes=plt.subplots(2,1,figsize=(20,8))<br><br>axes[0].hist(rating_num.values,bins=20)<br>axes[1].hist(runtime_num.values,bins=20)<br><br>#修改刻度<br>min_=data['Rating'].min()<br>max_=data['Rating'].max()<br>xticks=np.linspace(min_,max_,num=21)<br>axes[0].set_xticks(xticks)<br>min_=data['Runtime (Minutes)'].min()<br>max_=data['Runtime (Minutes)'].max()<br>xticks=np.linspace(min_,max_,num=21)<br>axes[1].set_xticks(xticks)<br><br>#标题<br>axes[0].set_title('Rating-hist')<br>axes[1].set_title('Runtime-hist')<br><br>#增加网格<br>axes[0].grid(linestyle='--',alpha=0.5)<br>axes[1].grid(linestyle='--',alpha=0.5)<br><br>plt.show()<br><br>#统计电影分类情况<br>genre=[i.split(',') for i in data['Genre'].values]<br>print(genre)<br>genre_unique=np.unique([i for j in genre for i in j])<br>print(genre_unique)<br><br>zeros=np.zeros((1000,genre_unique.size))<br>#创建一个DataFrame对象<br><br>genDF=pd.DataFrame(zeros,index=data['Title'].values,columns=genre_unique)<br>print(genDF.head())<br><br>print(np.unique(data.index.values).size)<br>for i in range(1000):<br>    for k in genre[i]:<br>        title=data['Title'][i]<br>        genDF[k][title]=1<br><br>gen_sum=genDF.sum().sort_values(ascending=False)<br>gen_sum.plot(kind='bar',figsize=(20,8),colormap='cool',fontsize=16)<br>plt.show()<br></pre><p data-we-empty-p=""><img src="https://img-blog.csdnimg.cn/3ba3bf7dbe144efbb888dcc6424dfb62.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbGl0dGxl5Lqu772e,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br><br></p><p data-we-empty-p=""><br></p><p data-we-empty-p=""><br></p><br><br><br><br><br><br><p data-we-empty-p="">​附件​(新增附件不会覆盖原有的)<br></p><hr><p data-we-empty-p="" class="CFP"><a href="http://www.lll.plus/static/file/2021/08/28/IMDB-Movie-Data.csv" target="_blank">IMDB-Movie-Data.csv</a><br></p><p data-we-empty-p="" class="CFP"><a href="http://www.lll.plus/static/file/2021/08/28/directory.csv" target="_blank">directory.csv</a><br></p>
