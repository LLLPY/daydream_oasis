---

next: false

---



<BlogInfo id="1120" title="41.爬取全网霉霉的图片" author="白日梦想猿" pv=0 read_times=0 pre_cost_time="2分26秒" category="爬虫学习" tag_list="['爬虫学习']" create_time="2021.07.13 13:46:59" update_time="2021.07.13 18:11:45" />

```python
from queue import Queue
from time import sleep
from requests import get
from selenium import webdriver
from threading import Thread
baseurl = 'https://image.baidu.com/search/index?tn=baiduimage&ipn=r&ct=201326592&cl=2&fm=detail&lm=-1&hd=undefined&latest=undefined&copyright=undefined&st=-1&sf=2&fmq=&fm=detail&pv=&ic=undefined&nc=1&z=0&se=&showtab=0&fb=0&width=undefined&height=undefined&face=0&istype=2&ie=utf-8&word=%E9%9C%89%E9%9C%89%E5%A3%81%E7%BA%B8'

class SpiderTS:
    count = 1  # 记录爬虫的数量
    urlQueue = Queue()  # 存放为爬取的url
    # 保存所有的url 不管有没有被爬取(功能是去重) 在将新获取的url添加到队列之前,检测集合中是否已经有该url,如果有就不添加到队列中
    urlDic = dict()

    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4503.5 Safari/537.36'}  # 请求头
        self.num = SpiderTS.count  # 每一个对象各自的编号
        if self.num == 1:
            print(f'编号{self.num}的爬虫正在获取图片的url中...')
        SpiderTS.count += 1  # 每新建一个对象-->爬虫数量加一

    def __str__(self):  # 爬取完成后的反馈
        if self.num == 1:
            return f'编号{self.num}的爬虫获取图片的url中完成!'
        else:
            return f'编号{self.num}的爬虫图片爬取完成!'

    # 生产者 不断获取图片的url,添加到urlQueue中
    def crawlUrl(self):
        chrome = webdriver.Chrome()
        chrome.get(baseurl)
        js = 'setInterval(function(){document.documentElement.scrollTop=10000000000000000},2000)'  # 将页面拉到底部
        while True:
            chrome.execute_script(js)
            imgUlrList = chrome.find_elements_by_xpath('//div[@class="imgpage"]/ul/li/div/a/img')  # 节点列表

            for img in imgUlrList:
                imgurl = img.get_attribute('data-imgurl')  # 节点的属性值
                if not self.check(imgurl):  # 如果是第一次获取的该url就加入到队列中
                    SpiderTS.urlQueue.put(imgurl)
                    SpiderTS.urlDic[imgurl] = 1  # 同时记录该url已被爬取

            sleep(1)

    # 检测某一条url 返回0就代表该url是第一次获取的
    def check(self, url):
        try:
            return SpiderTS.urlDic[url]
        except:
            return 0

    # 消费者 不断从urlQueue中获取url 进行图片的下载 下载完成之后 将该url添加到doneList中
    def downloadImg(self):
        url = SpiderTS.urlQueue.get()
        response = get(url, headers=self.headers)
        imgData = response.content

        with open(f'全网霉霉图片/TS{self.num}.jpg', 'wb+') as f:
            f.write(imgData)


# 创造生产者 并让其进行生产
def createProducer():
    # 生产者爬虫
    producer = SpiderTS()
    producer.crawlUrl()


# 创建消费者 并让其进行消费
def createCustomer():
    # 消费者爬虫
    while True:
        while SpiderTS.urlQueue.qsize() > 0:
            consumer = SpiderTS()
            consumer.downloadImg()
            print('剩余:', SpiderTS.urlQueue.qsize())


if __name__ == '__main__':
    # 使用多线程 让生产者和消费者同时进行各自的工作

    productThread = Thread(target=createProducer)
    consumeThread = Thread(target=createCustomer)

    productThread.start()
    consumeThread.start()

    productThread.join()
    consumeThread.join()

```



<ActionBox />
