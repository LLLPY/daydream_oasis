
<BlogInfo id="1102" title="25.爬取霉霉的图片" author="白日梦想猿" pv=0 read_times=0 pre_cost_time="1分39秒" category="爬虫学习" tag_list="['爬虫学习']" create_time="2020.06.02 16:38:46" update_time="2020.09.19 15:11:15" />

```python
#第一张
#http://sj.zol.com.cn/bizhi/showpic/480x800_84543_75.html
#最后一张
#http://sj.zol.com.cn/bizhi/showpic/480x800_84561_75.html


#http://desk.zol.com.cn/showpic/1920x1080_61557_63.html
#http://desk.zol.com.cn/showpic/1920x1080_61574_62.html

# import requests
# from fake_useragent import UserAgent
# import re
# headers = {
#     'User-Agent':UserAgent().random
# }

# #爬取1
# base_url = 'http://sj.zol.com.cn/bizhi/showpic/480x800_845{}_75.html'
# k = 1
# for i in range(43,62):
#     url = base_url.format(i)
#     response = requests.get(url,headers=headers)
#     contents = response.text
#     pattern = r'src="(.*)">'
#     imag_url = re.findall(pattern,contents)
#     # print(imag_url[0])
#     response = requests.get(imag_url[0],headers=headers)
#     contents = response.content
#     with open('爬取的数据\霉霉的图片\霉霉({}).jpg'.format(k),'wb') as f:
#         f.write(contents)
#         pass
#     k += 1
# print('爬取完成!')


# #爬取2
# base_url = 'http://desk.zol.com.cn/showpic/1920x1080_615{}_63.html'
# k = 1
# for i in range(57,75):
#     url = base_url.format(i)
#     response = requests.get(url,headers=headers)
#     contents = response.text
#     pattern = r'src="(.*)">'
#     imag_url = re.findall(pattern, contents)
#     print(imag_url)
#     response = requests.get(imag_url[0], headers=headers)
#     contents = response.content
#     #print(contents)
#     with open('\\Users\LLL\Pictures\霉霉\霉霉2.({}).jpg'.format(k),'wb') as f:
#         f.write(contents)
#         pass
#     k += 1
# print('done!')


#爬取3
from selenium import webdriver
from time import sleep
chrome = webdriver.Chrome()
chrome.get('https://www.ivsky.com/bizhi/taylor_swift_v25072/pic_476265.html')
k = 1
while True:
    sleep(0.5)
    #下载图片
    down_picture=chrome.find_element_by_xpath('//div/a[3][@target="_blank"]').click()
    sleep(0.5)
    #翻页
    next_page=chrome.find_element_by_xpath('//div/a[2][@class="page-next"]').click()
    sleep(1)
    k += 1
    print('第{}张图片下载完成!!!'.format(k))

```
