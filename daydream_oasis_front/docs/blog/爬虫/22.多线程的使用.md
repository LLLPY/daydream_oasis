
<BlogInfo title="22.多线程的使用" author="白日梦想猿" pv=0 read_times=0 pre_cost_time=1分15秒 category="爬虫学习" tag_list="['爬虫学习']" create_time="2020.06.01 17:11:04" update_time="2020.09.19 15:14:28" />

```python
from threading import Thread
from bs4 import BeautifulSoup
from fake_useragent import UserAgent
from queue import Queue
import requests
import time
#创建一个爬虫类，继承自Thread
class Spider_Thread(Thread):

    #初始化
    def __init__(self,url_queue):
        Thread.__init__(self)
        self.url_queue = url_queue

    #定义run方法
    def run(self):
        headers = {
            'User-agent':UserAgent().random
        }
        proxies = {
            'http':'http://94.103.95.59:3128',
            'https':'https://113.121.38.18:808'
        }
        #一直获取队列中的url一直爬取，直到队列为空!
        while url_queue.empty() == False:
            response = requests.get(url_queue.get(),headers=headers)
            contents = response.text
            contents = BeautifulSoup(contents,'lxml')
            contents = contents.dd.p.text
            print(contents)
            print('第{}页爬取完成...'.format(url_queue.qsize()))


if __name__ == '__main__':
    time1 = time.time()
    print('开始爬取...')
    #获取url
    #创建一个存储url的容器
    url_queue = Queue()
    # base_url = 'https://www.qiushibaike.com/text/page/{}/'
    base_url = 'http://qiushidabaike.com/index_{}'
    for i in range(1,51):
        new_url = base_url.format(i) #利用格式化字符函数将页面数据传入
        #将正确我的url保存在url队列中
        url_queue.put(new_url)


    #创建爬虫线程
    for i in range(1,6): #创建多只爬虫进行爬取
        spiders = Spider_Thread(url_queue)
        spiders.start()
        spiders.join()

    print('爬取完成!')
    time2 = time.time()
    print('耗时:{}'.format(time2-time1))
```
