---
sidebar: false
next: false
---
<BlogInfo/>






```python
from selenium import webdriver
from time import sleep
from lxml import etree
import requests
from fake_useragent import UserAgent
chrome = webdriver.Chrome()
chrome.get('https://m.bnmanhua.com/comic/16839/869201.html')
#sleep(1.5)
while True:
    contents = chrome.page_source
    e = etree.HTML(contents)
    manhua_url = e.xpath('//img[@id="comic_pic"]/@src')[0]
    title = e.xpath('//img[@id="comic_pic"]/@alt')[0]
    page_now = e.xpath('//*[@id="k_page"]/text()')[0]
    page_total = e.xpath('//*[@id="k_total"]/text()')[0]
    print(page_now)
    print(page_total)
    #下载漫画
    response = requests.get(manhua_url,headers={'User-agent':UserAgent().chrome})
    content = response.content
    with open('\\Users\LLL\Desktop\python\python基础(演练)\爬虫学习\scrapy学习\爬取的数据\斗罗大陆\{}.jpg'.format(title+(str(page_now))),'wb') as f:
        f.write(content)
    print(title, '(', page_now, ')', '爬取完成!!!')

    #翻页
    next_page = chrome.find_element_by_xpath('//div/a[@href="javascript:nextpage();"]').click()
    #睡眠1秒。等待网页加载
    sleep(1)

    if page_now == page_total:
        base_next_zhang = e.xpath('//a[@id="xurl"]/@href')[0] + '?p=1'
        next_zhang = 'https://m.bnmanhua.com' + base_next_zhang
        chrome.get(next_zhang)
        sleep(1)

    if manhua_url.find('.jpg') == -1:
        break


```






<ActionBox />
        
<style>#top-box {margin-top:0.5rem!important;}</style>