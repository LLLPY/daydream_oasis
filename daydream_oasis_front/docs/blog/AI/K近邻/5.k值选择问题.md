
<BlogInfo id="613" title="5.k值选择问题" author="白日梦想猿" pv=0 read_times=0 pre_cost_time=0分35秒 category="K近邻" tag_list="['K近邻']" create_time="2021.08.29 11:32:34" update_time="2021.08.29 11:35:08" />

```python
# k值选择
'''
K值选择问题，李航博士的一书「统计学习方法」上所说：

1) 选择较小的K值，就相当于用较小的领域中的训练实例进行预测，“学习”近似误差会减小，只有与输入实例
较近或相似的训练实例才会对预测结果起作用，与此同时带来的问题是“学习”的估计误差会增大，换句话说，K
值的减小就意味着整体模型变得复杂，容易发生过拟合；

2) 选择较大的K值，就相当于用较大领域中的训练实例进行预测，其优点是可以减少学习的估计误差，但缺点
是学习的近似误差会增大。这时候，与输入实例较远（不相似的）训练实例也会对预测器作用，使预测发生错误
，且K值的增大就意味着整体的模型变得简单。

3) K=N（N为训练样本个数），则完全不足取，因为此时无论输入实例是什么，都只是简单的预测它属于在训
练实例中最多的类，模型过于简单，忽略了训练实例中大量有用信息。

在实际应用中，K值一般取一个比较小的数值，例如采用交叉验证法（简单来说，就是把训练数据在分成两组:
训练集和验证集）来选择最优的K值。

近似误差：
    对现有训练集的训练误差，关注训练集，
    如果近似误差过小可能会出现过拟合的现象，对现有的训练集能有很好的预测，但是对未知的测试样本将会出现较大偏差的预测。
    模型本身不是最接近最佳模型。
估计误差：
    可以理解为对测试集的测试误差，关注测试集，
    估计误差小说明对未知数据的预测能力好，
    模型本身最接近最佳模型。


'''

# 小结
'''
K值过小：
    .容易受到异常点的影响
    .容易过拟合
k值过大：
    .受到样本均衡的问题
    .容易欠拟合


'''

```
