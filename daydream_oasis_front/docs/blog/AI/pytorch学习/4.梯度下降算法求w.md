
<BlogInfo title="4.梯度下降算法求w" author="白日梦想猿" pv=0 read_times=0 pre_cost_time=1分14秒 category="人工智能" tag_list="['人工智能']" create_time="2021.07.15 15:14:21" update_time="2021.07.15 16:14:24" />

```python
import numpy as np
import matplotlib.pyplot as plt
plt.rcParams['font.sans-serif'] = ['SimHei'] #用来正确显示中文

'''
梯度下降算法的核心是每一步都取当前的最优值(贪心法)

记:z=cost对w的偏导数 a称为学习率
公式:w=w-a*z 

具体步骤如下:
首先,在计算得到cost与其对应的w值后,然后从


'''

# 训练集
x_list = [i for i in np.arange(1.0, 4.0, 1.0)]
y_list = [i for i in np.arange(2.0, 8.0, 2.0)]


# 计数预测值的函数
def forward(x):
    return x * w


# 计算损失值的函数
def cost():
    sum_cost = 0
    for x, y in zip(x_list, y_list):
        pred_y = forward(x)
        sum_cost += (pred_y - y) ** 2
    return sum_cost


# 计算梯度的函数
def gradient():
    sum_grad = 0
    for x, y in zip(x_list, y_list):
        sum_grad += 2 * x * (forward(x) - y)
    return sum_grad / len(x_list)


if __name__ == '__main__':

    # w的初始值设为1.0 学习率的初始值为0.01
    w = 1.0
    a = 0.01

    print(f'Predicted (before training) f(4)={forward(4)}')

    loss_list=[]
    epoch_list=[i for i in range(1,101)] #训练的轮速
    w_list=[]
    for epoch in epoch_list:
        loss = cost()
        loss_list.append(loss)
        gradient_val=gradient()
        w_list.append(w)
        w -= a * gradient_val
        print(f'Epoch:{epoch} w={w} loss={loss} f(4)={forward(4)}')

    print(f'Predicted (after training) f(4)={forward(4)}')

    plt.subplot(2,1,1)
    plt.plot(epoch_list,loss_list)
    plt.title('损失值(loss)-轮数(epoch)')
    plt.xlabel('epoch')
    plt.ylabel('loss')
    plt.subplot(2,1,2)
    plt.title('损失值(loss)-w')
    plt.plot(w_list,loss_list)
    plt.xlabel('w')
    plt.ylabel('loss')
    plt.show()
```
